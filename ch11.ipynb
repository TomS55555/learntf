{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\"\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use bag of word model on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 80.2M    0  224k    0     0  87914      0  0:15:56  0:00:02  0:15:54 87984\n",
      "  0 80.2M    0  576k    0     0   162k      0  0:08:25  0:00:03  0:08:22  162k\n",
      "  1 80.2M    1 1024k    0     0   225k      0  0:06:04  0:00:04  0:06:00  225k\n",
      "  1 80.2M    1 1456k    0     0   262k      0  0:05:12  0:00:05  0:05:07  314k\n",
      "  2 80.2M    2 1920k    0     0   293k      0  0:04:40  0:00:06  0:04:34  384k\n",
      "  2 80.2M    2 2352k    0     0   311k      0  0:04:23  0:00:07  0:04:16  431k\n",
      "  3 80.2M    3 2864k    0     0   334k      0  0:04:05  0:00:08  0:03:57  457k\n",
      "  4 80.2M    4 3440k    0     0   360k      0  0:03:47  0:00:09  0:03:38  483k\n",
      "  5 80.2M    5 4144k    0     0   392k      0  0:03:29  0:00:10  0:03:19  537k\n",
      "  6 80.2M    6 5184k    0     0   448k      0  0:03:03  0:00:11  0:02:52  651k\n",
      "  7 80.2M    7 6096k    0     0   484k      0  0:02:49  0:00:12  0:02:37  744k\n",
      "  8 80.2M    8 6832k    0     0   504k      0  0:02:42  0:00:13  0:02:29  792k\n",
      "  9 80.2M    9 7408k    0     0   505k      0  0:02:42  0:00:14  0:02:28  778k\n",
      "  9 80.2M    9 7776k    0     0   499k      0  0:02:44  0:00:15  0:02:29  724k\n",
      "  9 80.2M    9 8112k    0     0   490k      0  0:02:47  0:00:16  0:02:31  586k\n",
      " 10 80.2M   10 8448k    0     0   481k      0  0:02:50  0:00:17  0:02:33  472k\n",
      " 10 80.2M   10 8784k    0     0   472k      0  0:02:53  0:00:18  0:02:35  388k\n",
      " 11 80.2M   11 9136k    0     0   467k      0  0:02:55  0:00:19  0:02:36  352k\n",
      " 11 80.2M   11 9536k    0     0   464k      0  0:02:56  0:00:20  0:02:36  353k\n",
      " 12 80.2M   12  9.8M    0     0   466k      0  0:02:56  0:00:21  0:02:35  387k\n",
      " 13 80.2M   13 10.4M    0     0   476k      0  0:02:52  0:00:22  0:02:30  457k\n",
      " 14 80.2M   14 11.3M    0     0   495k      0  0:02:45  0:00:23  0:02:22  579k\n",
      " 15 80.2M   15 12.1M    0     0   505k      0  0:02:42  0:00:24  0:02:18  653k\n",
      " 15 80.2M   15 12.7M    0     0   512k      0  0:02:40  0:00:25  0:02:15  713k\n",
      " 16 80.2M   16 13.5M    0     0   521k      0  0:02:37  0:00:26  0:02:11  757k\n",
      " 17 80.2M   17 14.2M    0     0   529k      0  0:02:35  0:00:27  0:02:08  771k\n",
      " 18 80.2M   18 14.9M    0     0   536k      0  0:02:33  0:00:28  0:02:05  732k\n",
      " 19 80.2M   19 15.7M    0     0   544k      0  0:02:30  0:00:29  0:02:01  737k\n",
      " 20 80.2M   20 16.5M    0     0   553k      0  0:02:28  0:00:30  0:01:58  758k\n",
      " 21 80.2M   21 17.1M    0     0   556k      0  0:02:27  0:00:31  0:01:56  747k\n",
      " 22 80.2M   22 17.9M    0     0   563k      0  0:02:25  0:00:32  0:01:53  747k\n",
      " 23 80.2M   23 18.7M    0     0   571k      0  0:02:23  0:00:33  0:01:50  767k\n",
      " 24 80.2M   24 19.5M    0     0   580k      0  0:02:21  0:00:34  0:01:47  789k\n",
      " 25 80.2M   25 20.4M    0     0   588k      0  0:02:19  0:00:35  0:01:44  802k\n",
      " 26 80.2M   26 21.2M    0     0   596k      0  0:02:17  0:00:36  0:01:41  844k\n",
      " 27 80.2M   27 22.1M    0     0   604k      0  0:02:15  0:00:37  0:01:38  871k\n",
      " 28 80.2M   28 23.1M    0     0   613k      0  0:02:13  0:00:38  0:01:35  900k\n",
      " 30 80.2M   30 24.1M    0     0   626k      0  0:02:11  0:00:39  0:01:32  945k\n",
      " 31 80.2M   31 25.4M    0     0   643k      0  0:02:07  0:00:40  0:01:27 1039k\n",
      " 33 80.2M   33 27.0M    0     0   667k      0  0:02:03  0:00:41  0:01:22 1187k\n",
      " 35 80.2M   35 28.7M    0     0   690k      0  0:01:58  0:00:42  0:01:16 1337k\n",
      " 37 80.2M   37 30.0M    0     0   706k      0  0:01:56  0:00:43  0:01:13 1414k\n",
      " 38 80.2M   38 31.0M    0     0   714k      0  0:01:55  0:00:44  0:01:11 1408k\n",
      " 39 80.2M   39 31.7M    0     0   713k      0  0:01:55  0:00:45  0:01:10 1276k\n",
      " 40 80.2M   40 32.4M    0     0   714k      0  0:01:55  0:00:46  0:01:09 1102k\n",
      " 41 80.2M   41 33.2M    0     0   715k      0  0:01:54  0:00:47  0:01:07  922k\n",
      " 42 80.2M   42 33.9M    0     0   716k      0  0:01:54  0:00:48  0:01:06  804k\n",
      " 43 80.2M   43 34.7M    0     0   717k      0  0:01:54  0:00:49  0:01:05  745k\n",
      " 44 80.2M   44 35.5M    0     0   719k      0  0:01:54  0:00:50  0:01:04  773k\n",
      " 45 80.2M   45 36.3M    0     0   722k      0  0:01:53  0:00:51  0:01:02  800k\n",
      " 46 80.2M   46 37.3M    0     0   728k      0  0:01:52  0:00:52  0:01:00  854k\n",
      " 48 80.2M   48 38.6M    0     0   738k      0  0:01:51  0:00:53  0:00:58  959k\n",
      " 49 80.2M   49 39.6M    0     0   742k      0  0:01:50  0:00:54  0:00:56  988k\n",
      " 51 80.2M   51 41.0M    0     0   756k      0  0:01:48  0:00:55  0:00:53 1134k\n",
      " 52 80.2M   52 41.9M    0     0   760k      0  0:01:48  0:00:56  0:00:52 1146k\n",
      " 53 80.2M   53 43.0M    0     0   765k      0  0:01:47  0:00:57  0:00:50 1152k\n",
      " 54 80.2M   54 44.0M    0     0   769k      0  0:01:46  0:00:58  0:00:48 1098k\n",
      " 56 80.2M   56 45.0M    0     0   774k      0  0:01:46  0:00:59  0:00:47 1130k\n",
      " 57 80.2M   57 46.0M    0     0   779k      0  0:01:45  0:01:00  0:00:45 1029k\n",
      " 58 80.2M   58 47.1M    0     0   784k      0  0:01:44  0:01:01  0:00:43 1053k\n",
      " 60 80.2M   60 48.2M    0     0   790k      0  0:01:43  0:01:02  0:00:41 1082k\n",
      " 61 80.2M   61 49.2M    0     0   794k      0  0:01:43  0:01:03  0:00:40 1081k\n",
      " 62 80.2M   62 50.2M    0     0   797k      0  0:01:43  0:01:04  0:00:39 1069k\n",
      " 63 80.2M   63 51.1M    0     0   798k      0  0:01:42  0:01:05  0:00:37 1037k\n",
      " 64 80.2M   64 52.0M    0     0   801k      0  0:01:42  0:01:06  0:00:36 1011k\n",
      " 66 80.2M   66 53.0M    0     0   803k      0  0:01:42  0:01:07  0:00:35  970k\n",
      " 67 80.2M   67 53.9M    0     0   805k      0  0:01:41  0:01:08  0:00:33  956k\n",
      " 68 80.2M   68 54.8M    0     0   808k      0  0:01:41  0:01:09  0:00:32  950k\n",
      " 69 80.2M   69 55.8M    0     0   811k      0  0:01:41  0:01:10  0:00:31  976k\n",
      " 70 80.2M   70 56.9M    0     0   815k      0  0:01:40  0:01:11  0:00:29 1000k\n",
      " 72 80.2M   72 58.1M    0     0   820k      0  0:01:40  0:01:12  0:00:28 1046k\n",
      " 74 80.2M   74 59.5M    0     0   829k      0  0:01:39  0:01:13  0:00:26 1152k\n",
      " 76 80.2M   76 60.9M    0     0   837k      0  0:01:38  0:01:14  0:00:24 1240k\n",
      " 76 80.2M   76 61.6M    0     0   835k      0  0:01:38  0:01:15  0:00:23 1174k\n",
      " 77 80.2M   77 62.1M    0     0   830k      0  0:01:38  0:01:16  0:00:22 1055k\n",
      " 78 80.2M   78 62.7M    0     0   827k      0  0:01:39  0:01:17  0:00:22  933k\n",
      " 78 80.2M   78 63.2M    0     0   824k      0  0:01:39  0:01:18  0:00:21  750k\n",
      " 79 80.2M   79 63.8M    0     0   821k      0  0:01:39  0:01:19  0:00:20  587k\n",
      " 80 80.2M   80 64.3M    0     0   818k      0  0:01:40  0:01:20  0:00:20  565k\n",
      " 81 80.2M   81 65.0M    0     0   817k      0  0:01:40  0:01:21  0:00:19  607k\n",
      " 81 80.2M   81 65.6M    0     0   814k      0  0:01:40  0:01:22  0:00:18  602k\n",
      " 82 80.2M   82 66.3M    0     0   812k      0  0:01:41  0:01:23  0:00:18  625k\n",
      " 83 80.2M   83 67.0M    0     0   811k      0  0:01:41  0:01:24  0:00:17  647k\n",
      " 84 80.2M   84 67.7M    0     0   810k      0  0:01:41  0:01:25  0:00:16  682k\n",
      " 85 80.2M   85 68.4M    0     0   809k      0  0:01:41  0:01:26  0:00:15  683k\n",
      " 86 80.2M   86 69.1M    0     0   809k      0  0:01:41  0:01:27  0:00:14  722k\n",
      " 87 80.2M   87 69.9M    0     0   808k      0  0:01:41  0:01:28  0:00:13  738k\n",
      " 88 80.2M   88 70.7M    0     0   809k      0  0:01:41  0:01:29  0:00:12  774k\n",
      " 89 80.2M   89 71.7M    0     0   811k      0  0:01:41  0:01:30  0:00:11  828k\n",
      " 90 80.2M   90 72.7M    0     0   813k      0  0:01:40  0:01:31  0:00:09  884k\n",
      " 91 80.2M   91 73.7M    0     0   816k      0  0:01:40  0:01:32  0:00:08  944k\n",
      " 93 80.2M   93 74.8M    0     0   819k      0  0:01:40  0:01:33  0:00:07 1009k\n",
      " 94 80.2M   94 75.7M    0     0   820k      0  0:01:40  0:01:34  0:00:06 1015k\n",
      " 95 80.2M   95 76.6M    0     0   821k      0  0:01:39  0:01:35  0:00:04 1004k\n",
      " 96 80.2M   96 77.6M    0     0   823k      0  0:01:39  0:01:36  0:00:03 1009k\n",
      " 98 80.2M   98 78.6M    0     0   825k      0  0:01:39  0:01:37  0:00:02  993k\n",
      " 99 80.2M   99 79.6M    0     0   827k      0  0:01:39  0:01:38  0:00:01  985k\n",
      "100 80.2M  100 80.2M    0     0   828k      0  0:01:39  0:01:39 --:--:-- 1001k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('aclImdb')\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir/category/fname, val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape=TensorShape([32])\n",
      "inputs.dtype=tf.string\n",
      "targets.shape=TensorShape([32])\n",
      "targets.dtype=tf.int32\n",
      "inputs[0]=<tf.Tensor: shape=(), dtype=string, numpy=b'A good idea, badly implemented. While that could summarize 99% of the SciFi channel\\'s movies, it really applies here. I love movies where a good back story is slowly revealed, and I like action movies, and I like all of the main actors, so this could have been great. However, despite some good acting, this movie fails due to Bill Platt\\'s bad writing and directing.<br /><br />Another review made the good point of needing to know where you\\'re going so you can get there. This movie doesn\\'t. It\\'s put together in such a haphazard way that you know the words \"second draft\" are not in Bill Platt\\'s vocabulary. There is one scene that is entirely unnecessary and could be removed without anyone noticing. This scene even begins and ends with them driving a car, so you could cut from one car scene to the other and never have missed the pointless scene in the middle.<br /><br />This movie also had a strange habit of under explaining some details while over explaining others, some to the point where you can guess the entire \"plot\" up front. It also had a habit of aborting a fight early, probably just because they couldn\\'t afford it. There are also a few laughably bad scenes where the \"plot\" is revealed on a computer and the final battle involving conveniently placed \"toxic adhesive\" (seriously, what *is* that?).<br /><br />If you are a fan of Shiri Appleby, watch this movie because she\\'s OK. She does manage to break out of her \"Roswell\" persona a few times and make for a good tough chick (but not always). John De Lancie plays the same character he plays in everything he\\'s ever done since playing Q back in ST:TNG, so that\\'s nothing new.<br /><br />In all, I gave this movie a 4/10 rating.'>\n",
      "targets[0]=<tf.Tensor: shape=(), dtype=int32, numpy=0>\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(f\"{inputs.shape=}\")\n",
    "    print(f\"{inputs.dtype=}\")\n",
    "    print(f\"{targets.shape=}\")\n",
    "    print(f\"{targets.dtype=}\")\n",
    "    print(f\"{inputs[0]=}\")\n",
    "    print(f\"{targets[0]=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encode the text using a bag-of-word model into binary vectors of the vocabulary size and ones where a word is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=2000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape=TensorShape([32, 2000])\n",
      "inputs.dtype=tf.int64\n",
      "targets.shape=TensorShape([32])\n",
      "targets.dtype=tf.int32\n",
      "inputs[0]=<tf.Tensor: shape=(2000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 0], dtype=int64)>\n",
      "targets[0]=<tf.Tensor: shape=(), dtype=int32, numpy=0>\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(f\"{inputs.shape=}\")\n",
    "    print(f\"{inputs.dtype=}\")\n",
    "    print(f\"{targets.shape=}\")\n",
    "    print(f\"{targets.dtype=}\")\n",
    "    print(f\"{inputs[0]=}\")\n",
    "    print(f\"{targets[0]=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
